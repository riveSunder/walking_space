{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fece05f6",
   "metadata": {},
   "source": [
    "# Machine Learning + Differentiable Physics \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/buggy_plot.png\" width=35%>\n",
    "</div>\n",
    "\n",
    "<h1> Demos:</h1>\n",
    "\n",
    "* <h2>How to get nonsensical output from your deep neural network (inspired by <a href=\"https://physicsbaseddeeplearning.com\">Physics Based Deep Learning Book</a>)</h2>\n",
    "\n",
    "<h3> a. How to make it behave better (by embedding the inverse problem).</h3>\n",
    "\n",
    "* <h2>Another way to get nonsensical output from your deep neural network (with out-of-distribution data)</h2>\n",
    "\n",
    "<h3> b. How to help your models tell you when they don't know anything about what they're telling you (by bootstrapping).</h3>\n",
    "\n",
    "\n",
    "| [Prev Page](page014_latent_space_map.ipynb) | [Next Page](page0xy_differentiable_physics.ipynb) | \n",
    "|------------------------------------------------------------|------------------------------------|\n",
    "| | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d5fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.d_physics import get_data, forward_nn, forward_nn_loss, \\\n",
    "        get_layers, train_nn, train_dp, forward_dp_loss, bootstrap, \\\n",
    "        plot_parabola\n",
    "\n",
    "from autograd import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c69615",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training steps\n",
    "max_steps = 2500\n",
    "\n",
    "\"\"\"\n",
    "Get training data, a model (layers and biases), and check \n",
    "starting performance on a validation dataset\n",
    "\"\"\"\n",
    "x, y = get_data(batch_size = 2048)\n",
    "layers, biases = get_layers()\n",
    "\n",
    "val_x, val_y = get_data(batch_size = 128)\n",
    "pred_y = forward_nn(val_x, layers, biases)\n",
    "plot_parabola(val_x, val_y, pred_y, my_title=\"NN-Only Before Training\")\n",
    "\n",
    "\"\"\"\n",
    "Train the model and validate\n",
    "\"\"\"\n",
    "layers, biases = train_nn(layers, biases, x, y, max_steps=max_steps)\n",
    "val_x, val_y = get_data(batch_size = 128, stretch=2.0)\n",
    "\n",
    "pred_y = forward_nn(val_x, layers, biases)\n",
    "plot_parabola(val_x, val_y, pred_y, my_title=\"NN-Only After Training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c30975",
   "metadata": {},
   "source": [
    "# Pre-Computed Results\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/nn_precomputed.png\" width=90%>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18dc5ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now train the same neural network architecture, but with the inverse\n",
    "problem embedded in the loss function. This alleviates the bi-modal \n",
    "confusion from the first example. \n",
    "\"\"\"\n",
    "\n",
    "max_steps = 3000\n",
    "\n",
    "\"\"\"\n",
    "Initialize a model (layers and biases), and check \n",
    "starting performance on a validation dataset\n",
    "\"\"\"\n",
    "layers, biases = get_layers()\n",
    "val_x, val_y = get_data(batch_size = 128)\n",
    "\n",
    "pred_y = forward_nn(val_x, layers, biases)\n",
    "plot_parabola(val_x, val_y, pred_y, my_title=\"D. Physics Before Training\")\n",
    "\n",
    "\"\"\"\n",
    "Train the model and validate\n",
    "\"\"\"\n",
    "layers, biases = train_dp(layers, biases, x, y, max_steps=max_steps)\n",
    "val_x, val_y = get_data(batch_size = 128, stretch=2.0)\n",
    "\n",
    "pred_y = forward_nn(val_x, layers, biases)\n",
    "plot_parabola(val_x, val_y, pred_y, my_title=\"D. Physics After Training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3688c58",
   "metadata": {},
   "source": [
    "# Pre-Computed Results\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/dp_precomputed.png\" width=90%>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d385c7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Even taking advantage of our prior knowledge of the problem,\n",
    "the model performs poorly with out of distribution data.\n",
    "We can train multiple models with data subsets and use \n",
    "their (dis)agreement to estimate (un)certainty.\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(314)\n",
    "val_x, val_y, pred, pred_std_dev, l_layers, l_biases = bootstrap(get_layers, get_data, num_straps=3)\n",
    "\n",
    "plot_parabola(val_x, val_y, pred, pred_range=pred_std_dev, my_title=\"Diff. Physics w/ Bootstrap Uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c79fe",
   "metadata": {},
   "source": [
    "# Pre-Computed Result\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"assets/uncertainty_example.png\">\n",
    "</div>\n",
    "    \n",
    "(mean of bootstrapped ensemble predictions +/- standard deviation)\n",
    "\n",
    "The bootstrap example was inspired by \n",
    "* Osband, Ian _et al._ “Deep Exploration via Bootstrapped DQN.” NIPS (2016). <a href=\"https://arxiv.org/abs/1602.04621\">https://arxiv.org/abs/1602.04621</a>\n",
    "\n",
    "While the parabola problem example was inspired by \n",
    "\n",
    "* N. Thuerey, P. Holl, M. Mueller, P. Schnell, F. Trost, K. Um. Physics-based Deep Learning.\n",
    "    <a href=\"https://physicsbaseddeeplearning.org\">https://physicsbaseddeeplearning.org</a> 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_parabola(mode=0, batch_size=128, stretch=1.):\n",
    "    \n",
    "    x = np.random.rand(batch_size, 1) * stretch\n",
    "    \n",
    "    if mode == 0:\n",
    "        y = np.sign(np.random.randn(batch_size))[:, np.newaxis] * np.sqrt(x)\n",
    "        \n",
    "    elif mode == 1:\n",
    "        y = -1.0 * np.sqrt(x)\n",
    "        \n",
    "    elif mode == 2:\n",
    "        y = np.sqrt(x)\n",
    "        \n",
    "    return x, y\n",
    "\n",
    "def forward_nn(x, layers, biases):\n",
    "    \n",
    "    #activation = lambda x: x * (x > 0) + x * 0.1 * (x < 0)\n",
    "    #activation = lambda x: x**2\n",
    "    activation = lambda x: anp.sin(x**2)\n",
    "    #activation = lambda x: anp.tanh(x)\n",
    "    \n",
    "    for layer, bias in zip(layers, biases):\n",
    "        \n",
    "        x = activation(anp.matmul(x, layer)) + bias\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "def forward_nn_loss(x, y_target, layers, biases):\n",
    "        \n",
    "    prediction = forward_nn(x, layers, biases)\n",
    "    \n",
    "    loss = anp.mean(anp.abs(y_target - prediction)**2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "    \n",
    "def get_layers(dim_x=1, dim_h=32, dim_y=1, number_h=1):\n",
    "    \n",
    "    layers = []\n",
    "    biases = []\n",
    "    \n",
    "    layers.append(30 / (dim_x * dim_h) * np.random.randn(dim_x, dim_h))\n",
    "    biases.append(np.random.randn(dim_h))\n",
    "    \n",
    "\n",
    "    for ii in range(number_h):\n",
    "        layers.append(30 / (dim_h**2) * np.random.randn(dim_h, dim_h))\n",
    "        biases.append(np.random.randn(dim_h))\n",
    "\n",
    "    layers.append(30 / (dim_h * dim_y) * np.random.randn(dim_h, dim_y))\n",
    "    biases.append(np.random.randn(dim_y))\n",
    "    \n",
    "    return layers, biases\n",
    "\n",
    "def train_nn(layers, biases,  x, y, max_steps=2000, lr=3e-4):\n",
    "    \n",
    "    for ii in range(max_steps):\n",
    "\n",
    "\n",
    "        if ii % (max_steps // 10) == 0:\n",
    "\n",
    "            loss = forward_nn_loss(x, y, layers, biases)\n",
    "\n",
    "            print(f\"loss at step {ii} = {loss:.4}\")\n",
    "\n",
    "        grad_layers, grad_biases = get_nn_grad(x, y, layers, biases)\n",
    "\n",
    "        for params, grads in zip(layers, grad_layers):\n",
    "            params -=  lr * grads\n",
    "\n",
    "        for params, grads in zip(biases, grad_biases):\n",
    "            params -=  lr * grads\n",
    "            \n",
    "    return layers, biases\n",
    "\n",
    "def forward_dp_loss(x, layers, biases):\n",
    "        \n",
    "    prediction = forward_nn(x, layers, biases)\n",
    "    \n",
    "    loss = anp.mean(anp.abs(x - prediction**2)**2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "get_dp_grad = grad(forward_dp_loss, argnum=(1,2))\n",
    "\n",
    "def train_dp(layers, biases, x, y, max_steps=2000, lr=1e-4):\n",
    "    \n",
    "    for ii in range(max_steps):\n",
    "\n",
    "        if ii % (max_steps // 10) == 0:\n",
    "\n",
    "            loss = forward_dp_loss(x, layers, biases)\n",
    "\n",
    "            print(f\"loss at step {ii} = {loss:.4}\")\n",
    "\n",
    "        grad_layers, grad_biases = get_dp_grad(x, layers, biases)\n",
    "\n",
    "        for params, grads in zip(layers, grad_layers):\n",
    "            params -=  lr * grads\n",
    "\n",
    "        for params, grads in zip(biases, grad_biases):\n",
    "            params -=  lr * grads\n",
    "            \n",
    "    return layers, biases\n",
    "\n",
    "def bootstrap(layers_fn, data_fn, num_straps=3):\n",
    "    \n",
    "    x, y = data_fn(batch_size=256)\n",
    "    \n",
    "    l_layers = []\n",
    "    l_biases = []\n",
    "    \n",
    "    for ii in range(num_straps):\n",
    "        print(f\"begin training bootstrap {ii}\")\n",
    "        l, b = layers_fn()\n",
    "        \n",
    "        train_dp(l, b, x, y, max_steps=3000)\n",
    "        \n",
    "        l_layers.append(l)\n",
    "        l_biases.append(b)\n",
    "    \n",
    "    val_x, val_y = data_fn(batch_size=512, stretch=2.0)\n",
    "    \n",
    "    preds_x = None\n",
    "    \n",
    "    for jj in range(num_straps):\n",
    "        \n",
    "        preds_x = forward_nn(val_x, l_layers[jj], l_biases[jj]) if preds_x is None \\\n",
    "                else np.append(preds_x, forward_nn(val_x, l_layers[jj], l_biases[jj]), axis=-1)\n",
    "        \n",
    "    # The np.sign here is a little bit cheating... ensures that all predictions refer to same mode\n",
    "    pred_x = np.mean(preds_x * np.sign(preds_x), axis=-1)\n",
    "    \n",
    "    std_dev_pred = np.std(preds_x* np.sign(preds_x), axis=-1)\n",
    "        \n",
    "    return val_x, val_y, pred_x, std_dev_pred, l_layers, l_biases    \n",
    "\n",
    "def plot_parabola(x, y, pred, pred_range=None, my_title=\"Parabola Figure\"):\n",
    "    \n",
    "    my_cmap = plt.get_cmap(\"magma\")\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    plt.scatter(x, y, color=my_cmap(50), label=\"target\")\n",
    "    plt.scatter(x, pred, color=my_cmap(150), label=\"prediction\")\n",
    "    plt.plot([1., 1.0000000001], [-2.50, 2.50], \"--\", alpha=0.45, \\\n",
    "             color=[0,0,0], label=\"training boundary\")\n",
    "    \n",
    "    if pred_range is not None:\n",
    "        #import pdb; pdb.set_trace()\n",
    "        x = x.squeeze()\n",
    "        pred = pred.squeeze()\n",
    "        pred_range = pred_range.squeeze()\n",
    "        x_args = np.argsort(x)\n",
    "        plt.fill_between(x[x_args], (pred+pred_range)[x_args], \\\n",
    "                         (pred-pred_range)[x_args], alpha=0.5, color=my_cmap(150))\n",
    "        \n",
    "    plt.legend(fontsize=20)\n",
    "    plt.title(my_title, fontsize=24)\n",
    "    plt.show()\n",
    "\n",
    "get_nn_grad = grad(forward_nn_loss, argnum=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef2b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
