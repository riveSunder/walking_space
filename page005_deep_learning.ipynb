{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c3d3ed",
   "metadata": {},
   "source": [
    "# What is Deep Learning?\n",
    "\n",
    "* <h2><a href=\"https://en.wikipedia.org/wiki/Universal_approximation_theorem\">The Universal Approximation Theorem</a> states that a single hidden layer with non-linear activation is sufficient to represent any arbitrary (well-behaved) function [0][1].</h2>\n",
    "\n",
    "<!-- * <h2>Common consensus at 7 layers? </h2> -->\n",
    "\n",
    "* <h2>Schmidhuber [2] suggests 10 layers (or credit assignment pathways) as requiring \"very deep\" learning.</h2>\n",
    "\n",
    "* <h2>My opinion: \"deep double descent\" empirically seperates deep and shallow learning.</h2>\n",
    "\n",
    "<img src=\"./assets/deep_double_descent_rive.png\" width=80%>\n",
    "\n",
    "[0] Cybenko 1989\n",
    "\n",
    "[1] Hornik 1989\n",
    "\n",
    "[2] Schmidhuber J. Deep learning in neural networks: an overview. Neural Networks. (2015) [https://arxiv.org/pdf/1404.7828v1.pdf](https://arxiv.org/pdf/1404.7828v1.pdf)\n",
    "\n",
    "[3] Nakkiran, Preetum et al. \"Deep Double Descent: Where Bigger Models and More Data Hurt.\" (2020) [https://arxiv.org/abs/1912.02292](https://arxiv.org/abs/1912.02292)\n",
    "\n",
    "Training curve from my blog post [The Shapes of Regularization](https://rivesunder.github.io/SortaSota/machine_learning/regularization/2021/01/12/lnorm_regularization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45e63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
